\chapter{Entropy}
\begin{definition}[Entropy]
	A measure of uncertainty of a physical system.
	\[
		H(x) = H(p_1,p_2,\ldots p_n ) = -\sum_x p_x \log p_x
	\]
	\[
		\lim\limits_{p \to 0} p\log p = 0
	\]
\end{definition}
X - Information we gain, on an average when we learn the value of X.
\begin{eg}
	Coin toss :- HHHH - H, if it gives only heads, Information gain is zero.
\end{eg}
\subsection*{Operational interpretation of entropy}
Entropy is tied to memory resources. 
\begin{eg}
	X  takes values \((x_1, x_2 , x_3, x_4)\) with probability \((\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8})\)\\ encoding them with \((0,10,110,111)\) \(\implies \frac{1}{2}[1]+\frac{1}{4}[2]+\frac{1}{8}[3]+\frac{1}{8}[3] = \frac{7}{4}\)bits
	\[
		-\sum\limits_{x=1}^{4} p_x \log p_x = \frac{7}{4} \text{bits}
	\]
\end{eg}
\begin{eg}
	For a coin \(p_H = 1\) and \(p_T = 0\) size of memory = 0
\end{eg}
\subsection*{Entropy from intuitive axioms}
\begin{enumerate}
	\item I(p)
	\item I(p) is smooth
	\item I(pq) = I(p)+I(q)
\end{enumerate}
\subsection*{Properties of Entropy}
\[
	H_{bin}(p) = -p\log p -(1-p)\log (1-p) 
\]
get a quadratic curve
\begin{figure}[H]
	\centering
	\incfig{quad}
	\caption{title}
	\label{fig:quad}
\end{figure}
\[
	H(q p_I +(1-q)p_N) \geq qH(p_I)+(1-q)H(p_N)
\]
\[
	f(px + (1-p)y)\geq pf(x)+(1-p)f(y)
\]
\subsection*{Relative Entropy}
\begin{definition}
	\[
		H(p(x)\mid \mid q(x)) = - \sum\limits_{x=1}^{n} p(x) \log \frac{q(x)}{p(x)} 
	\]
\end{definition}
\begin{theorem}
	\[
		H(p(x)\mid \mid q(x)) = \sum p(x) \log \frac{p(x)}{q(x)} \text{is non-negative }
	\]
	\[
		= 0 \text{ iff p(x) = q(x) for all x}
	\]
\end{theorem}