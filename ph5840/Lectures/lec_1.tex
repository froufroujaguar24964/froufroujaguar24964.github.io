\chapter{Entropy}
\begin{definition}[Entropy]
	A measure of uncertainty of a physical system.
	\[
		H(x) = H(p_1,p_2,\ldots p_n ) = -\sum_x p_x \log p_x
	\]
	\[
		\lim\limits_{p \to 0} p\log p = 0
	\]
\end{definition}
X - Information we gain, on an average when we learn the value of X.
\begin{eg}
	Coin toss :- HHHH - H, if it gives only heads, Information gain is zero.
\end{eg}
\subsection*{Operational interpretation of entropy}
Entropy is tied to memory resources. 
\begin{eg}
	X  takes values \((x_1, x_2 , x_3, x_4)\) with probability \((\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8})\)\\ encoding them with \((0,10,110,111)\) \(\implies \frac{1}{2}[1]+\frac{1}{4}[2]+\frac{1}{8}[3]+\frac{1}{8}[3] = \frac{7}{4}\)bits
	\[
		-\sum\limits_{x=1}^{4} p_x \log p_x = \frac{7}{4} \text{bits}
	\]
\end{eg}
\begin{eg}
	For a coin \(p_H = 1\) and \(p_T = 0\) size of memory = 0
\end{eg}
\subsection*{Entropy from intuitive axioms}
\begin{enumerate}
	\item I(p)
	\item I(p) is smooth
	\item I(pq) = I(p)+I(q)
\end{enumerate}